# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/data.ipynb.

# %% auto 0
__all__ = ['TYPE2INT', 'INT2TYPE', 'DataCollatorCTCWithPadding', 'BenetechDataset']

# %% ../nbs/data.ipynb 1
import pandas as pd
import numpy as np
import torch
import cv2
import os

from transformers import Pix2StructProcessor
from torch.utils.data import Dataset
from dataclasses import dataclass

# %% ../nbs/data.ipynb 2
TYPE2INT = {
    "scatter": 0, 
    "line": 1, 
    "vertical_bar": 2, 
    "dot": 3, 
    "horizontal_bar": 4
}

INT2TYPE = {v: k for k, v in TYPE2INT.items()}

# %% ../nbs/data.ipynb 3
@dataclass
class DataCollatorCTCWithPadding:

    processor: Pix2StructProcessor

    def __call__(self, batch):
        
        new_batch = {"flattened_patches":[], "attention_mask":[], "target_type":[], "target_text":[], "target_text_x":[], "target_text_y":[], "id":[], "x_axis_type":[], "y_axis_type":[]}
        
        for item in batch:
            new_batch["flattened_patches"].append(item["flattened_patches"])
            new_batch["attention_mask"].append(item["attention_mask"])
            new_batch["target_type"].append(item["target_type"])
            new_batch["target_text"].append(item["target_text"])
            new_batch["target_text_x"].append(item["target_text_x"])
            new_batch["target_text_y"].append(item["target_text_y"])
            new_batch["id"].append(item["id"])
            new_batch["x_axis_type"].append(item["x_axis_type"])
            new_batch["y_axis_type"].append(item["y_axis_type"])
            
        labels = [{"input_ids": b["labels"]} for b in batch]
        
        labels_padded = self.processor.tokenizer.pad(
            labels,
            padding="longest",
            return_tensors="pt")
        
        new_batch["labels"] = labels_padded.pop("input_ids").masked_fill(labels_padded["attention_mask"].ne(1), -100)
        new_batch["labels_attention_mask"] = labels_padded.pop("attention_mask")
        new_batch["attention_mask"] = torch.stack(new_batch["attention_mask"])
        new_batch["flattened_patches"] = torch.stack(new_batch["flattened_patches"])
        new_batch["target_type"] = torch.stack(new_batch["target_type"])

        return new_batch

# %% ../nbs/data.ipynb 4
class BenetechDataset(Dataset):
    
    def __init__(self, dataset_df, processor, mode="train_val", max_patches=1024, transforms=None):
        super().__init__()
        
        assert mode in ("train_val", "inference"), "mode must be train_val or infernce"
        self.mode = mode
        self.max_patches = max_patches
        
        self.transforms = transforms
        self.processor = processor
        self.dataset_df = dataset_df

        self.pretokenized = "tokenized_text" in self.dataset_df.columns

    def __len__(self):
        return len(self.dataset_df)
    
    def tokenize_text(self, text):
        return np.squeeze(self.processor.tokenizer.encode(text, \
            padding="longest", return_tensors="pt", add_special_tokens=True))
        
    def __getitem__(self, idx):
        image_path = self.dataset_df.at[idx, "image_path"]
        image_id = os.path.basename(image_path).replace(".jpg", "")
        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)
        
        if self.transforms is not None:
            image = self.transforms(image=image)
                       
        encoding = self.processor(images=image, add_special_tokens=True, return_tensors="pt", max_patches=self.max_patches)
               
        output_dict = {
            "flattened_patches": encoding["flattened_patches"][0],
            "attention_mask": encoding["attention_mask"][0],
            "id": image_id}

        if self.mode == "train_val":
            
            target_text = self.dataset_df.at[idx, "target_text"]

            labels = self.dataset_df.at[idx, "tokenized_text"] if self.pretokenized \
                else self.tokenize_text(f"{target_text}")

            output_dict["target_text"] = target_text
            
            output_dict["target_text_x"] = self.dataset_df.at[idx, "target_text_x"]
            output_dict["target_text_y"] = self.dataset_df.at[idx, "target_text_y"]
            
            output_dict["x_axis_type"] = self.dataset_df.at[idx, "x_axis_type"]
            output_dict["y_axis_type"] = self.dataset_df.at[idx, "y_axis_type"]
            
            output_dict["labels"] = labels
            output_dict["target_type"] = torch.tensor(TYPE2INT[self.dataset_df.at[idx, "target_type"]], dtype=torch.long)

        return output_dict 
        

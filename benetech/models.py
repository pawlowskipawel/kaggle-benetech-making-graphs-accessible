# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/models.ipynb.

# %% auto 0
__all__ = ['DeplotWithClassificationHead']

# %% ../nbs/models.ipynb 1
from transformers import Pix2StructForConditionalGeneration
import torch

# %% ../nbs/models.ipynb 2
class DeplotWithClassificationHead(torch.nn.Module):
    def __init__(self, backbone):
        super().__init__()
        
        self.deplot = Pix2StructForConditionalGeneration.from_pretrained(backbone)
        self.deplot.config.text_config.is_decoder=True
        
        self.dropout = torch.nn.Dropout(.4)
        self.classifiaction_head = torch.nn.Linear(768, 5)
        
        
    def forward(self, flattened_patches, attention_mask, labels=None, decoder_input_ids=None, decoder_attention_mask=None):
        deplot_output = self.deplot.forward(flattened_patches=flattened_patches, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, labels=labels)
        encoder_mean = deplot_output["encoder_last_hidden_state"].mean(1)
        deplot_output["type_logits"] = self.classifiaction_head(self.dropout(encoder_mean))

        return deplot_output
    
    @torch.inference_mode()
    def generate(self, flattened_patches, attention_mask, max_new_tokens=128):
        output = self.deplot.generate(flattened_patches=flattened_patches, attention_mask=attention_mask, return_dict_in_generate=True, output_hidden_states=True, max_new_tokens=max_new_tokens)
        
        sequences = output["sequences"]
        types = self.classifiaction_head(output["encoder_hidden_states"][-1].mean(1)).argmax(1).cpu().numpy().tolist()
        
        return sequences, types
        

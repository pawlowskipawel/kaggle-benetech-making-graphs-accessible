# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/metrics.ipynb.

# %% auto 0
__all__ = ['sigmoid', 'normalized_rmse', 'normalized_levenshtein_score', 'score_series', 'benetech_score', 'BenetechMetric',
           'AccuracyMetric']

# %% ../nbs/metrics.ipynb 1
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from rapidfuzz.distance.Levenshtein import distance as levenshtein
from sklearn.metrics import r2_score, accuracy_score

# %% ../nbs/metrics.ipynb 2
def sigmoid(x):
    return 2 - 2 / (1 + np.exp(-x))


def normalized_rmse(y_true, y_pred):
    # The argument to the sigmoid transform is equal to 
    # rmse(y_true, y_pred) / rmse(y_true, np.mean(y_true))
    return sigmoid((1 - r2_score(y_true, y_pred)) ** 0.5)


def normalized_levenshtein_score(y_true, y_pred):
    total_distance = np.sum([levenshtein(yt, yp) for yt, yp in zip(y_true, y_pred)])
    length_sum = np.sum([len(yt) for yt in y_true])
    return sigmoid(total_distance / length_sum)


def score_series(y_true, y_pred):
    if len(y_true) != len(y_pred):
        return 0.0
    if isinstance(y_true[0], str):
        return normalized_levenshtein_score(y_true, y_pred)
    else:
        return normalized_rmse(y_true, y_pred)


def benetech_score(ground_truth: pd.DataFrame, predictions: pd.DataFrame) -> float:
    """Evaluate predictions using the metric from the Benetech - Making Graphs Accessible.
    
    Parameters
    ----------
    ground_truth: pd.DataFrame
        Has columns `[data_series, chart_type]` and an index `id`. Values in `data_series` 
        should be either arrays of floats or arrays of strings.
    
    predictions: pd.DataFrame
    """
    if not ground_truth.index.equals(predictions.index):
        raise ValueError("Must have exactly one prediction for each ground-truth instance.")
    if not ground_truth.columns.equals(predictions.columns):
        raise ValueError(f"Predictions must have columns: {ground_truth.columns}.")
    pairs = zip(ground_truth.itertuples(index=False), predictions.itertuples(index=False))
    scores = []
    for (gt_series, gt_type), (pred_series, pred_type) in pairs:
        if gt_type != pred_type:  # Check chart_type condition
            scores.append(0.0)
        else:  # Score with RMSE or Levenshtein as appropriate
            scores.append(score_series(gt_series, pred_series))
    return np.mean(scores)

# %% ../nbs/metrics.ipynb 3
class BenetechMetric:
    def __init__(self):
        super().__init__()

        self.labels_text = []
        self.predictions_text = []

        self.prediction_type = []
        self.labels_type = []

        self.axis_types = []        

        self.current_metric = 0
        
    def reset(self):
        self.labels_text = []
        self.predictions_text = []

        self.prediction_type = []
        self.labels_type = []

        self.axis_types = []        

        self.current_metric = 0
        
    def update(self, predictions_text, labels_text, prediction_type, labels_type, axis_types):
        
        self.predictions_text.extend(predictions_text)
        self.labels_text.extend(labels_text)

        self.prediction_type.extend(prediction_type)
        self.labels_type.extend(labels_type)
        
        self.axis_types.extend(axis_types)
        
        
    def update_types(self, string, data_type):
        string = string.split(";")
        
        if data_type == "categorical":
            return string
        
        if data_type == "numerical":
            try:
                return [float(x) for x in string]
            except Exception:
                return []
        
    
    def compute(self):

        self.gt_df = pd.DataFrame({"data_series": self.labels_text,
                              "chart_type": self.labels_type,
                              "axis_type": self.axis_types})
        
        self.pred_df = pd.DataFrame({"data_series": self.predictions_text,
                                "chart_type": self.prediction_type,
                                "axis_type": self.axis_types})
        
        self.gt_df["data_series"] = np.where(self.gt_df["axis_type"] == "categorical", self.gt_df["data_series"].apply(lambda x: self.update_types(x, "categorical")), \
            self.gt_df["data_series"].apply(lambda x: self.update_types(x, "numerical")))
        
        self.pred_df["data_series"] = np.where(self.pred_df["axis_type"] == "categorical", self.pred_df["data_series"].apply(lambda x: self.update_types(x, "categorical")), \
            self.pred_df["data_series"].apply(lambda x: self.update_types(x, "numerical")))

        self.current_metric = benetech_score(ground_truth=self.gt_df.drop("axis_type", axis=1), predictions=self.pred_df.drop("axis_type", axis=1))

        return self.current_metric
    
    def get_metric(self):
        return self.current_metric

# %% ../nbs/metrics.ipynb 4
class AccuracyMetric:
    def __init__(self):
        super().__init__()

        self.prediction_type = []
        self.labels_type = []

        self.current_metric = 0
        
    def reset(self):
        self.prediction_type = []
        self.labels_type = []

        self.current_metric = 0
        
    def update(self, predictions_text=None, labels_text=None, prediction_type=None, labels_type=None, axis_types=None):

        self.prediction_type.extend(prediction_type)
        self.labels_type.extend(labels_type)
        
    def compute(self):

        self.current_metric = accuracy_score(self.labels_type, self.prediction_type)
        
        return self.current_metric
    
    def get_metric(self):
        return self.current_metric
